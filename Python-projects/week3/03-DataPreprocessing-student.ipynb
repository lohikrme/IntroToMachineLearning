{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Data preprocessing - Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Identifying missing values in tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A     B     C     D\n",
       "0    1.0   2.0   3.0   4.0\n",
       "1    5.2   6.0   NaN   9.0\n",
       "2    6.0   7.0   NaN  10.3\n",
       "3    7.0   8.0   9.0  11.0\n",
       "4    8.0   9.0   NaN   NaN\n",
       "5    9.0  10.2  11.0   NaN\n",
       "6   15.0  11.0   NaN  14.0\n",
       "7   16.0  12.0  13.0   NaN\n",
       "8   17.5  13.0   NaN  16.0\n",
       "9   18.0  14.0  15.0  17.1\n",
       "10  20.0  15.0  18.0   NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create dummy dataset for training\n",
    "csv_data = \\\n",
    "'''A,B,C,D\n",
    " 1.0, 2.0,3.0,4.0\n",
    " 5.2, 6.0,,9.0\n",
    " 6.0, 7.0,,10.3\n",
    " 7.0, 8.0,9.0,11.0\n",
    " 8.0, 9.0,,\n",
    " 9.0,10.2,11.0,\n",
    "15.0,11.0,,14.0\n",
    "16.0,12.0,13.0,\n",
    "17.5,13.0,,16.0\n",
    "18.0,14.0,15.0,17.1\n",
    "20.0,15.0,18.0,'''\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.154545</td>\n",
       "      <td>9.745455</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>11.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.329512</td>\n",
       "      <td>3.851328</td>\n",
       "      <td>5.205766</td>\n",
       "      <td>4.508035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.750000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               A          B          C          D\n",
       "count  11.000000  11.000000   6.000000   7.000000\n",
       "mean   11.154545   9.745455  11.500000  11.628571\n",
       "std     6.329512   3.851328   5.205766   4.508035\n",
       "min     1.000000   2.000000   3.000000   4.000000\n",
       "25%     6.500000   7.500000   9.500000   9.650000\n",
       "50%     9.000000  10.200000  12.000000  11.000000\n",
       "75%    16.750000  12.500000  14.500000  15.000000\n",
       "max    20.000000  15.000000  18.000000  17.100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overview of the dataset\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A      B      C      D\n",
      "0   False  False  False  False\n",
      "1   False  False   True  False\n",
      "2   False  False   True  False\n",
      "3   False  False  False  False\n",
      "4   False  False   True   True\n",
      "5   False  False  False   True\n",
      "6   False  False   True  False\n",
      "7   False  False  False   True\n",
      "8   False  False   True  False\n",
      "9   False  False  False  False\n",
      "10  False  False  False   True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A    0\n",
       "B    0\n",
       "C    5\n",
       "D    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out number of missing values in the dataset\n",
    "print(df.isnull())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. ,  3. ,  4. ],\n",
       "       [ 5.2,  6. ,  nan,  9. ],\n",
       "       [ 6. ,  7. ,  nan, 10.3],\n",
       "       [ 7. ,  8. ,  9. , 11. ],\n",
       "       [ 8. ,  9. ,  nan,  nan],\n",
       "       [ 9. , 10.2, 11. ,  nan],\n",
       "       [15. , 11. ,  nan, 14. ],\n",
       "       [16. , 12. , 13. ,  nan],\n",
       "       [17.5, 13. ,  nan, 16. ],\n",
       "       [18. , 14. , 15. , 17.1],\n",
       "       [20. , 15. , 18. ,  nan]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the values that are in the numpy array to inform missing values\n",
    "# (access the underlying NumPy array via the `values` attribute)\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Eliminating training samples or features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A     B     C     D\n",
      "0    1.0   2.0   3.0   4.0\n",
      "1    5.2   6.0   NaN   9.0\n",
      "2    6.0   7.0   NaN  10.3\n",
      "3    7.0   8.0   9.0  11.0\n",
      "4    8.0   9.0   NaN   NaN\n",
      "5    9.0  10.2  11.0   NaN\n",
      "6   15.0  11.0   NaN  14.0\n",
      "7   16.0  12.0  13.0   NaN\n",
      "8   17.5  13.0   NaN  16.0\n",
      "9   18.0  14.0  15.0  17.1\n",
      "10  20.0  15.0  18.0   NaN\n",
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "3   7.0   8.0   9.0  11.0\n",
      "9  18.0  14.0  15.0  17.1\n"
     ]
    }
   ],
   "source": [
    "# Find out which rows don't have missing values\n",
    "# because axis 0 = by row, axis 1 = by column\n",
    "# these basically show only rows that dont have any NaNs\n",
    "print(df)\n",
    "print(df.dropna(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A     B\n",
      "0    1.0   2.0\n",
      "1    5.2   6.0\n",
      "2    6.0   7.0\n",
      "3    7.0   8.0\n",
      "4    8.0   9.0\n",
      "5    9.0  10.2\n",
      "6   15.0  11.0\n",
      "7   16.0  12.0\n",
      "8   17.5  13.0\n",
      "9   18.0  14.0\n",
      "10  20.0  15.0\n"
     ]
    }
   ],
   "source": [
    "# Find out which columns don't have missing values\n",
    "print(df.dropna(axis=1, how=\"any\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A     B     C     D\n",
      "1    NaN   NaN   NaN   NaN\n",
      "2    1.0   2.0   3.0   4.0\n",
      "3    5.2   6.0   NaN   9.0\n",
      "4    6.0   7.0   NaN  10.3\n",
      "5    7.0   8.0   9.0  11.0\n",
      "6    8.0   9.0   NaN   NaN\n",
      "7    9.0  10.2  11.0   NaN\n",
      "8   15.0  11.0   NaN  14.0\n",
      "9   16.0  12.0  13.0   NaN\n",
      "10  17.5  13.0   NaN  16.0\n",
      "11  18.0  14.0  15.0  17.1\n",
      "12  20.0  15.0  18.0   NaN\n",
      "14   NaN   NaN   NaN   NaN\n",
      "\n",
      "       A     B     C     D\n",
      "2    1.0   2.0   3.0   4.0\n",
      "3    5.2   6.0   NaN   9.0\n",
      "4    6.0   7.0   NaN  10.3\n",
      "5    7.0   8.0   9.0  11.0\n",
      "6    8.0   9.0   NaN   NaN\n",
      "7    9.0  10.2  11.0   NaN\n",
      "8   15.0  11.0   NaN  14.0\n",
      "9   16.0  12.0  13.0   NaN\n",
      "10  17.5  13.0   NaN  16.0\n",
      "11  18.0  14.0  15.0  17.1\n",
      "12  20.0  15.0  18.0   NaN\n"
     ]
    }
   ],
   "source": [
    "# How to drop rows where all columns are NaN\n",
    "# because current data does not have a row with only NaN values,\n",
    "# i will first add a new row of data with just NaN values\n",
    "import copy\n",
    "\n",
    "length = 4\n",
    "# create an array of NaNs\n",
    "nan_array = np.full(length, np.nan)\n",
    "# create deep copy of existing df to not mess up it\n",
    "df2 = copy.deepcopy(df)\n",
    "# use loc to add a rows of NaN\n",
    "df2.loc[-1] = nan_array\n",
    "df2.index = df2.index + 1\n",
    "df2 = df2.sort_index()\n",
    "df2.loc[len(df2) + 1] = nan_array\n",
    "df2.index = df2.index + 1\n",
    "df2 = df2.sort_index()\n",
    "print(df2)\n",
    "print()\n",
    "\n",
    "# now print again df2, drop all rows of only NaN\n",
    "print(df2.dropna(axis=0, how=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A     B     C     D\n",
      "0    1.0   2.0   3.0   4.0\n",
      "1    5.2   6.0   NaN   9.0\n",
      "2    6.0   7.0   NaN  10.3\n",
      "3    7.0   8.0   9.0  11.0\n",
      "4    8.0   9.0   NaN   NaN\n",
      "5    9.0  10.2  11.0   NaN\n",
      "6   15.0  11.0   NaN  14.0\n",
      "7   16.0  12.0  13.0   NaN\n",
      "8   17.5  13.0   NaN  16.0\n",
      "9   18.0  14.0  15.0  17.1\n",
      "10  20.0  15.0  18.0   NaN\n",
      "\n",
      "       A     B     C     D\n",
      "0    1.0   2.0   3.0   4.0\n",
      "1    5.2   6.0   NaN   9.0\n",
      "2    6.0   7.0   NaN  10.3\n",
      "3    7.0   8.0   9.0  11.0\n",
      "5    9.0  10.2  11.0   NaN\n",
      "6   15.0  11.0   NaN  14.0\n",
      "7   16.0  12.0  13.0   NaN\n",
      "8   17.5  13.0   NaN  16.0\n",
      "9   18.0  14.0  15.0  17.1\n",
      "10  20.0  15.0  18.0   NaN\n"
     ]
    }
   ],
   "source": [
    "# drop rows that have fewer than 3 real values \n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(df.dropna(thresh=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "1   5.2   6.0   NaN   9.0\n",
      "2   6.0   7.0   NaN  10.3\n",
      "3   7.0   8.0   9.0  11.0\n",
      "6  15.0  11.0   NaN  14.0\n",
      "8  17.5  13.0   NaN  16.0\n",
      "9  18.0  14.0  15.0  17.1\n"
     ]
    }
   ],
   "source": [
    "# only drop rows where NaN appear in specific columns (here: 'D')\n",
    "print(df.dropna(axis=0, subset=[\"D\"], how=\"any\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. ,  3. ,  4. ],\n",
       "       [ 5.2,  6. ,  nan,  9. ],\n",
       "       [ 6. ,  7. ,  nan, 10.3],\n",
       "       [ 7. ,  8. ,  9. , 11. ],\n",
       "       [ 8. ,  9. ,  nan,  nan],\n",
       "       [ 9. , 10.2, 11. ,  nan],\n",
       "       [15. , 11. ,  nan, 14. ],\n",
       "       [16. , 12. , 13. ,  nan],\n",
       "       [17.5, 13. ,  nan, 16. ],\n",
       "       [18. , 14. , 15. , 17.1],\n",
       "       [20. , 15. , 18. ,  nan]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again: original array\n",
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    11.154545\n",
      "B     9.745455\n",
      "C    11.500000\n",
      "D    11.628571\n",
      "dtype: float64\n",
      "\n",
      "A     9.0\n",
      "B    10.2\n",
      "C    12.0\n",
      "D    11.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find out column stats\n",
    "print(df.mean())\n",
    "print()\n",
    "print(df.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.   nan  9. ]\n",
      " [ 6.   7.   nan 10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.   nan  nan]\n",
      " [ 9.  10.2 11.   nan]\n",
      " [15.  11.   nan 14. ]\n",
      " [16.  12.  13.   nan]\n",
      " [17.5 13.   nan 16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.   nan]]\n",
      "\n",
      "SimpleImputer()\n",
      "\n",
      "[[ 1.          2.          3.          4.        ]\n",
      " [ 5.2         6.         11.5         9.        ]\n",
      " [ 6.          7.         11.5        10.3       ]\n",
      " [ 7.          8.          9.         11.        ]\n",
      " [ 8.          9.         11.5        11.62857143]\n",
      " [ 9.         10.2        11.         11.62857143]\n",
      " [15.         11.         11.5        14.        ]\n",
      " [16.         12.         13.         11.62857143]\n",
      " [17.5        13.         11.5        16.        ]\n",
      " [18.         14.         15.         17.1       ]\n",
      " [20.         15.         18.         11.62857143]]\n"
     ]
    }
   ],
   "source": [
    "# impute missing values via the column mean\n",
    "# so replace the not a number values with mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(df.values); print()\n",
    "imr_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imr_mean.fit(df.values)\n",
    "print(imr_mean); print()\n",
    "mean_inputed_data = imr_mean.transform(df.values)\n",
    "print(mean_inputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.   nan  9. ]\n",
      " [ 6.   7.   nan 10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.   nan  nan]\n",
      " [ 9.  10.2 11.   nan]\n",
      " [15.  11.   nan 14. ]\n",
      " [16.  12.  13.   nan]\n",
      " [17.5 13.   nan 16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.   nan]]\n",
      "\n",
      "[[ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.  12.   9. ]\n",
      " [ 6.   7.  12.  10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.  12.  11. ]\n",
      " [ 9.  10.2 11.  11. ]\n",
      " [15.  11.  12.  14. ]\n",
      " [16.  12.  13.  11. ]\n",
      " [17.5 13.  12.  16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.  11. ]]\n"
     ]
    }
   ],
   "source": [
    "# impute missing values via the column median\n",
    "print(df.values); print()\n",
    "imr_median = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "imr_median.fit(df.values)\n",
    "median_inputed_data = imr_median.transform(df.values)\n",
    "print(median_inputed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan  nan  nan]\n",
      " [ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.   nan  9. ]\n",
      " [ 6.   7.   nan 10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.   nan  nan]\n",
      " [ 9.  10.2 11.   nan]\n",
      " [15.  11.   nan 14. ]\n",
      " [16.  12.  13.   nan]\n",
      " [17.5 13.   nan 16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.   nan]\n",
      " [ nan  nan  nan  nan]]\n",
      "\n",
      "[[ nan  nan  nan  nan]\n",
      " [ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.   3.   9. ]\n",
      " [ 6.   7.   3.  10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.   9.  11. ]\n",
      " [ 9.  10.2 11.  11. ]\n",
      " [15.  11.  11.  14. ]\n",
      " [16.  12.  13.  14. ]\n",
      " [17.5 13.  13.  16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.  17.1]\n",
      " [20.  15.  18.  17.1]]\n"
     ]
    }
   ],
   "source": [
    "# impute missing values using LOCF\n",
    "print(df2.values); print()\n",
    "df3 = df2.ffill(inplace = False)\n",
    "print(df3.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan  nan  nan]\n",
      " [ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.   nan  9. ]\n",
      " [ 6.   7.   nan 10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.   nan  nan]\n",
      " [ 9.  10.2 11.   nan]\n",
      " [15.  11.   nan 14. ]\n",
      " [16.  12.  13.   nan]\n",
      " [17.5 13.   nan 16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.   nan]\n",
      " [ nan  nan  nan  nan]]\n",
      "\n",
      "[[ 1.   2.   3.   4. ]\n",
      " [ 1.   2.   3.   4. ]\n",
      " [ 5.2  6.   9.   9. ]\n",
      " [ 6.   7.   9.  10.3]\n",
      " [ 7.   8.   9.  11. ]\n",
      " [ 8.   9.  11.  14. ]\n",
      " [ 9.  10.2 11.  14. ]\n",
      " [15.  11.  13.  14. ]\n",
      " [16.  12.  13.  16. ]\n",
      " [17.5 13.  15.  16. ]\n",
      " [18.  14.  15.  17.1]\n",
      " [20.  15.  18.   nan]\n",
      " [ nan  nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "# impute missing values using NOCB\n",
    "print(df2.values); print()\n",
    "df3 = df2.bfill(inplace = False)\n",
    "print(df3.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Handling categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Nominal and ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     color sizeletter  price  classlabel\n",
      "0    green          M   10.1  sweatshirt\n",
      "1      red          L   13.5     t-shirt\n",
      "2    white          S   13.5     t-shirt\n",
      "3   yellow          M   13.5  sweatshirt\n",
      "4     pink          M   13.5  sweatshirt\n",
      "5    black          S   13.5     t-shirt\n",
      "6     grey          S   13.5     t-shirt\n",
      "7     grey          S   13.5    pullover\n",
      "8     grey          L   13.5     t-shirt\n",
      "9   orange          M   13.5    pullover\n",
      "10    lila          L   13.5    pullover\n",
      "11    blue         XL   15.3    pullover\n"
     ]
    }
   ],
   "source": [
    "# create dummy dataset for categorial data\n",
    "df = pd.DataFrame([['green', 'M', 10.1, 'sweatshirt'],\n",
    "                   ['red',   'L', 13.5, 't-shirt'],\n",
    "                   ['white', 'S', 13.5, 't-shirt'],\n",
    "                   ['yellow','M', 13.5, 'sweatshirt'],\n",
    "                   ['pink',  'M', 13.5, 'sweatshirt'],\n",
    "                   ['black', 'S', 13.5, 't-shirt'],\n",
    "                   ['grey',  'S', 13.5, 't-shirt'],\n",
    "                   ['grey',  'S', 13.5, 'pullover'],\n",
    "                   ['grey',  'L', 13.5, 't-shirt'],\n",
    "                   ['orange','M', 13.5, 'pullover'],\n",
    "                   ['lila',  'L', 13.5, 'pullover'],\n",
    "                   ['blue', 'XL', 15.3, 'pullover']])\n",
    "\n",
    "df.columns = ['color', 'sizeletter', 'price', 'classlabel']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out different categories in the dataset\n",
    "# color and classlabel are nominal, sizeletter is ordinal category\n",
    "# this assessment must be done manually as far as i know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mapping ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     color sizeletter  price  classlabel  size_code  color_code  label_code\n",
      "0    green          M   10.1  sweatshirt          2           2           1\n",
      "1      red          L   13.5     t-shirt          3           7           2\n",
      "2    white          S   13.5     t-shirt          1           8           2\n",
      "3   yellow          M   13.5  sweatshirt          2           9           1\n",
      "4     pink          M   13.5  sweatshirt          2           6           1\n",
      "5    black          S   13.5     t-shirt          1           0           2\n",
      "6     grey          S   13.5     t-shirt          1           3           2\n",
      "7     grey          S   13.5    pullover          1           3           0\n",
      "8     grey          L   13.5     t-shirt          3           3           2\n",
      "9   orange          M   13.5    pullover          2           5           0\n",
      "10    lila          L   13.5    pullover          3           4           0\n",
      "11    blue         XL   15.3    pullover          4           1           0\n"
     ]
    }
   ],
   "source": [
    "# first save the ordinal category size_code into df\n",
    "# define mapping dictionary S=1, M=2, L=3 and XL=4 and add size_code column to the dataset\n",
    "size_mapping = {'XL': 4,\n",
    "                'L': 3,\n",
    "                'M': 2,\n",
    "                'S': 1}\n",
    "df[\"size_code\"] = df[\"sizeletter\"].map(size_mapping)\n",
    "\n",
    "# next save the nominal categories color_code and label_code into df\n",
    "df[\"color_code\"] = df.color.astype(\"category\").cat.codes\n",
    "df[\"label_code\"] = df.classlabel.astype(\"category\").cat.codes\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('XL', 4), ('L', 3), ('M', 2), ('S', 1)])\n",
      "dict_items([(4, 'XL'), (3, 'L'), (2, 'M'), (1, 'S')])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      M\n",
       "1      L\n",
       "2      S\n",
       "3      M\n",
       "4      M\n",
       "5      S\n",
       "6      S\n",
       "7      S\n",
       "8      L\n",
       "9      M\n",
       "10     L\n",
       "11    XL\n",
       "Name: size_code, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define inverse mapping function\n",
    "# idea is to create a python dictionary, where every number equals a size\n",
    "print(size_mapping.items())\n",
    "inv_size_mapping = {value: key for key, value in size_mapping.items()}\n",
    "print(inv_size_mapping.items())\n",
    "df['size_code'].map(inv_size_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('XL', 4), ('L', 3), ('M', 2), ('S', 1)])\n",
      "dict_items([(4, 'XL'), (3, 'L'), (2, 'M'), (1, 'S')])\n"
     ]
    }
   ],
   "source": [
    "# Find out inv size mapping dictionary\n",
    "print(size_mapping.items())\n",
    "print(inv_size_mapping.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Encoding class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     color sizeletter  price  classlabel  size_code\n",
      "0    green          M   10.1  sweatshirt          2\n",
      "1      red          L   13.5     t-shirt          3\n",
      "2    white          S   13.5     t-shirt          1\n",
      "3   yellow          M   13.5  sweatshirt          2\n",
      "4     pink          M   13.5  sweatshirt          2\n",
      "5    black          S   13.5     t-shirt          1\n",
      "6     grey          S   13.5     t-shirt          1\n",
      "7     grey          S   13.5    pullover          1\n",
      "8     grey          L   13.5     t-shirt          3\n",
      "9   orange          M   13.5    pullover          2\n",
      "10    lila          L   13.5    pullover          3\n",
      "11    blue         XL   15.3    pullover          4\n",
      "\n",
      "\n",
      "     color sizeletter  price  classlabel  size_code  color_code  label_code\n",
      "0    green          M   10.1  sweatshirt          2           2           1\n",
      "1      red          L   13.5     t-shirt          3           7           2\n",
      "2    white          S   13.5     t-shirt          1           8           2\n",
      "3   yellow          M   13.5  sweatshirt          2           9           1\n",
      "4     pink          M   13.5  sweatshirt          2           6           1\n",
      "5    black          S   13.5     t-shirt          1           0           2\n",
      "6     grey          S   13.5     t-shirt          1           3           2\n",
      "7     grey          S   13.5    pullover          1           3           0\n",
      "8     grey          L   13.5     t-shirt          3           3           2\n",
      "9   orange          M   13.5    pullover          2           5           0\n",
      "10    lila          L   13.5    pullover          3           4           0\n",
      "11    blue         XL   15.3    pullover          4           1           0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import copy\n",
    "\n",
    "# u need a separate encoder for each category\n",
    "color_encoder = LabelEncoder()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Label encoding with sklearn's LabelEncoder\n",
    "# first take a deepcopy of dataframe\n",
    "df2 = copy.deepcopy(df)\n",
    "#print(df2)\n",
    "# remove 2 columns and recreate them with sklearn library\n",
    "df2 = df2.drop(labels=[\"color_code\", \"label_code\"], axis=1)\n",
    "print(df2); print(); print()\n",
    "\n",
    "# fit_transform combines fit and transform phases\n",
    "# fit phase encoder learns how data looks from categorical labels point of view\n",
    "# transform phase it creates an array of equivalent numbers to categories\n",
    "df2[\"color_code\"] = color_encoder.fit_transform(df2['color'])\n",
    "df2[\"label_code\"] = label_encoder.fit_transform(df2[\"classlabel\"])\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colors of data: ['green' 'red' 'white' 'yellow' 'pink' 'black' 'grey' 'grey' 'grey'\n",
      " 'orange' 'lila' 'blue']\n",
      "label classes of data: ['sweatshirt' 't-shirt' 't-shirt' 'sweatshirt' 'sweatshirt' 't-shirt'\n",
      " 't-shirt' 'pullover' 't-shirt' 'pullover' 'pullover' 'pullover']\n"
     ]
    }
   ],
   "source": [
    "# reverse mapping\n",
    "# now that label encoder has been used, we can use its \n",
    "# inverse_transform function to easily revert code numbers to equivalent category keys\n",
    "\n",
    "print(f\"colors of data: {color_encoder.inverse_transform(df2[\"color_code\"])}\")\n",
    "print(f\"label classes of data: {label_encoder.inverse_transform(df2[\"label_code\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Performing one-hot encoding on nominal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2\n",
      "1     7\n",
      "2     8\n",
      "3     9\n",
      "4     6\n",
      "5     0\n",
      "6     3\n",
      "7     3\n",
      "8     3\n",
      "9     5\n",
      "10    4\n",
      "11    1\n",
      "Name: color_code, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# encode the color column using LabelEncoder\n",
    "\n",
    "# i think i alrdy did this...\n",
    "print(df2[\"color_code\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['green' 'M' 10.1]\n",
      " ['red' 'L' 13.5]\n",
      " ['white' 'S' 13.5]\n",
      " ['yellow' 'M' 13.5]\n",
      " ['pink' 'M' 13.5]\n",
      " ['black' 'S' 13.5]\n",
      " ['grey' 'S' 13.5]\n",
      " ['grey' 'S' 13.5]\n",
      " ['grey' 'L' 13.5]\n",
      " ['orange' 'M' 13.5]\n",
      " ['lila' 'L' 13.5]\n",
      " ['blue' 'XL' 15.3]]\n",
      "\n",
      "['green' 'M' 10.1]\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the color column\n",
    "# one hot encoding means, each category has its own column\n",
    "# and then, 1 means something is that color\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe_colors = OneHotEncoder()\n",
    "\n",
    "X = df2[['color', 'sizeletter', 'price']].values\n",
    "print(X); print()\n",
    "print(X[0])\n",
    "\n",
    "# encoded_colors = ohe_colors.fit_transform(X[[\"color\"]]).toarray()\n",
    "# print(encoded_colors)\n",
    "\n",
    "#encoded_df2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding via pandas get_dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multicollinearity guard in get_dummies\n",
    "# Multicollinearity arises when two or more variables in a regression are highly correlated\n",
    "# removes the 1st categorial value (black is now a row with all the other colors as zero)\n",
    "# (Note! dropping the first dummy variable can also lead to a slight loss in interpretability, \n",
    "# as you'll have to infer the dropped category based on the values of the remaining columns.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Partitioning a dataset into a seperate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wine dataset\n",
    "# https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
    "# df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',header=None)\n",
    "\n",
    "# if the Wine dataset is temporarily unavailable from the\n",
    "# UCI machine learning repository, un-comment the following line\n",
    "# of code to load the dataset from a local path:\n",
    "\n",
    "# df_wine = pd.read_csv('wine.data', header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "print('Class labels', np.unique(df_wine['Class label']))\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wine dataset overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out unique classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test datasets (ensure that distributions are preserved)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "    test_size=0.3, random_state=0, stratify=y)\n",
    "print(\"Shapes: {} = {} + {}\".format(X.shape, X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Bringing features onto the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Selecting meaningful features\n",
    "\n",
    "Assessing feature importance with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_labels = df_wine.columns[1:]\n",
    "forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train.shape[1]), \n",
    "        importances[indices],\n",
    "        align='center')\n",
    "\n",
    "plt.xticks(range(X_train.shape[1]), \n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/04_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(forest, threshold=0.1, prefit=True)\n",
    "X_selected = sfm.transform(X_train)\n",
    "print('Number of features that meet this threshold criterion:', \n",
    "      X_selected.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 3 features that met the threshold criterion for feature selection that was set earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(X_selected.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "093fe19a2dfd657596699a6307754b478c03c45d083fb45e7e73a1436017c754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
